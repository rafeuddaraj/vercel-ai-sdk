Previously, we built AI powered text completion where users can send a prompt, wait for the entire response, and then see it all at once. But here's the thing, AI models can be slow, especially when generating longer responses. Users might be staring at a loading spinner for 5, 10, even 20 seconds, which is not a great experience. This is where streaming comes in. Instead of waiting for the entire response, we can start showing text as soon as the AI starts generating it. It's like the difference between

downloading a movie completely before watching it versus streaming it on YouTube or Netflix. With streaming, the user sees progress immediately, which makes the app feel much faster and more responsive. For this lesson, let's see how we can implement streaming to improve the user experience in our NexJS app. First, let's create our streaming API route. So go to the app folder and then API folder and create a new folder called stream. Inside this folder, create a route.ts file. The route

handler code is very similar to the completion route handler code, but we will write this from scratch to make sure the concepts stick. Start by defining an async post request handler. So export async function post. The function name must be post as this tells NexJS which HTTP method this handler responds to. Next, let's add our imports at the top of the file. import a function called stream text from the core AI package and OpenAI from AI SDK/ OpenAI. Stream text is a function designed specifically for streaming

responses. Now inside the post function, let's get the prompt from the request. So request of type request and within the function body, we dstructure prompt by awaiting quest.json. Now let's call stream text with our model and prompt. So stream text parenthesis object property is model. we specify open AAI and our model is GPD 4.1 nano and in the next line we specify prompt set to prompt of course we can make use of esex shorthand and specify just prompt we will store the return value in

a constant called result and it is very important we don't await stream text finally we return the streaming response so return result dot to UI message stream response. This method to UI message stream response creates an HTTP response that streams the data in a format that the UI can understand. Finally, let's wrap this in a try catch block to handle any errors. So try catch. We catch an error console. error streaming text and then we return a new response failed to stream text status 500. Our streaming route is

ready. You can see how similar this is to our previous route. The main differences are using stream text instead of generate text and returning to UI message stream response instead of JSON. Now for the exciting part. Let's build a UI component that can consume this streaming response. So in the UI folder, create a new folder called stream. And inside stream, create page.tsx. Let's start with the basic component structure. Use client directive since this is going to be a client component.

export default function stream page with a simple div that says stream page for the JSX. We will replicate the UI from our previous completion page. We'll start with a container div. We'll leave space for the streaming text to be displayed. So display area for streaming text will go here. At the bottom goes the form. So form a flex div container with input and a send button the input placeholder. How can I help? The last bit is to style these elements using tailwind classes. And you can copy

the styles from completion page.tsx. I also realized stream needs to be inside the UI folder. And here stream needs to be inside the API folder. To save us the time I have pasted classes for each element behind the scenes. You can see this UI in the browser by navigating to / UI/ream. The same layout as the completion component, but I've changed the placeholder to ask me anything to make it clear. The next step is to make the input interactive by adding state. The beauty of the AI SDK is that it provides

a hook specifically for handling streaming completions. Let's import it at the top. import use completion from AI SDK/react. Now let's use this hook in our component. Use completion. This hook needs to know which API endpoint to call. So we pass in an object with the API property set to / API/stream. This is the path of our route handler. This hook returns an object with everything we need to handle the streaming response. First let's get the input value and the onchange handler. So

input and handle input change. Let's bind these to our input field. value is equal to input on change is equal to handle input change. Next, let's get the form submission handler. So after handle input change, the structure handle submit bind it to onsubmit on the form element. Onsubmit is equal to handle submit. Now for the most important part, the streaming completion. Let's get it once again from the hook. It's called completion. The completion value automatically updates as new chunks

arrive from the server. It's displayed above the form. So if the completion exists, render it in a div. The magic here is that we don't need to manually handle the stream or update state. The hook does it all for us. Next, let's add loading state. The use completion hub provides ease loading property. Restructure it and use it to disable the button while loading. So disabled is equal to e is loading. Let's also add a loading indicator that shows when we are waiting for the first chunk.

So e is loading and there is no completion render loading text. Next let's add error handling. The hook provides an error object. Restructure it. We can use this to display errors when they occur. So error and if it exists, we display error dot message. We'll also apply some tail classes. Text red 500 and margin bottom four. All right, time to test this in the browser. Navigate to /ui/stream and enter a prompt. What is an LLM? When you submit, you should see the loading text appear and the response stream in

real time. This is a great improvement over the previous example where the user had to wait for the entire response before seeing any text. Now, as of this recording, there is a bug where the input is not cleared after you submit a prompt. This is a known issue and should be fixed soon, but here's a workar around for now. from use completion destructure the set input function in the onsubmit handler call it with an empty string. So this is going to be a function where we prevent default behavior call set input with an empty

string and then call handle submit passing in the event. Back in the browser enter a prompt what is machine learning. Press enter and you should see the input cleared and the response stream in real time. Let me show you one more feature related to streaming. For really long responses, users might want to stop the generation. The use completion hook gives us a stop function which we can use to abort the current API request. So destructure the stop function from the hook and update the submit button to show a stop button when

the stream is in progress. So curly braces for some JavaScript is loading. We render a stop button. On click is equal to stop and we're going to add some tailwind classes. Again styling is not the focus of this course. So please feel free to style the buttons as you like. Now if it is not loading, we render the current send button. Save the file to format the turnary expression. Back in the browser, start a long prompt by entering explain machine learning in detail and while it is streaming, click

the stop button. This will halt the streaming response. We have successfully implemented streaming text generation. All right, let's quickly review what we've done. We created a streaming API route using stream text and two UI message stream response method. We built a UI with the use completion hook. We added all the features one by one. Input handling, streaming display, loading state, error handling, and the stop functionality. The AI SDK handles all the complex streaming protocols, letting

us focus on building great user experiences. If something's not working, the code is available on my GitHub repo. So, please make sure to check that out. All right, for our next topic, we'll take a step back and learn about models and tokens, which are fundamental concepts to understand when working with AI.

